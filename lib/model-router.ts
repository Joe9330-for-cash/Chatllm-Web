import { SupportedModel } from '../pages/api/chat';

interface ModelConfig {
  temperature: number;
  max_tokens: number;
  timeout: number;
  retry: number;
  top_p?: number;
  frequency_penalty?: number;
  presence_penalty?: number;
  stream_options?: any;
  flushThreshold?: number;
  flushInterval?: number;
}

interface ContentAnalysis {
  length: number;
  language: 'chinese' | 'english' | 'mixed';
  hasCode: boolean;
  hasLongText: boolean;
  isCreative: boolean;
  isAnalytical: boolean;
  complexity: 'low' | 'medium' | 'high';
}

class ModelRouter {
  private static instance: ModelRouter;
  private performanceCache: Map<string, { model: SupportedModel; score: number; timestamp: number }> = new Map();
  private readonly cacheExpiryMs = 30 * 60 * 1000; // 30åˆ†é’Ÿç¼“å­˜è¿‡æœŸ

  private constructor() {}

  static getInstance(): ModelRouter {
    if (!ModelRouter.instance) {
      ModelRouter.instance = new ModelRouter();
    }
    return ModelRouter.instance;
  }

  /**
   * åˆ†æè¾“å…¥å†…å®¹ç‰¹å¾
   */
  private analyzeContent(content: string): ContentAnalysis {
    const length = content.length;
    
    // æ£€æµ‹è¯­è¨€
    const chineseChars = (content.match(/[\u4e00-\u9fa5]/g) || []).length;
    const englishChars = (content.match(/[a-zA-Z]/g) || []).length;
    const language = chineseChars > englishChars ? 'chinese' : 
                    englishChars > chineseChars ? 'english' : 'mixed';
    
    // æ£€æµ‹ä»£ç 
    const codePatterns = [
      /function\s+\w+\s*\(/,
      /class\s+\w+/,
      /import\s+.*from/,
      /```[\s\S]*?```/,
      /\w+\s*=\s*function/,
      /\w+\s*=\s*\(.*\)\s*=>/,
      /if\s*\(.*\)\s*{/,
      /for\s*\(.*\)\s*{/,
      /while\s*\(.*\)\s*{/,
      /def\s+\w+\(/,
      /print\s*\(/,
      /console\.log\s*\(/,
      /SELECT\s+.*FROM/i,
      /CREATE\s+TABLE/i,
      /INSERT\s+INTO/i,
      /UPDATE\s+.*SET/i,
      /DELETE\s+FROM/i,
      /<\w+[^>]*>/,
      /\{\{.*\}\}/,
      /\$\{.*\}/,
    ];
    const hasCode = codePatterns.some(pattern => pattern.test(content));
    
    // æ£€æµ‹é•¿æ–‡æœ¬
    const hasLongText = length > 500;
    
    // æ£€æµ‹åˆ›æ„æ€§å†…å®¹
    const creativeKeywords = [
      'åˆ›ä½œ', 'å†™ä½œ', 'æ•…äº‹', 'è¯—æ­Œ', 'å°è¯´', 'å‰§æœ¬', 'åˆ›æ„', 'æƒ³è±¡', 'è™šæ„',
      'write', 'story', 'creative', 'imagine', 'fiction', 'poetry', 'novel',
      'å†™ä¸€ä¸ª', 'ç¼–å†™', 'åˆ›é€ ', 'è®¾è®¡', 'æ„æ€', 'å‘æŒ¥', 'å¹»æƒ³'
    ];
    const isCreative = creativeKeywords.some(keyword => content.includes(keyword));
    
    // æ£€æµ‹åˆ†ææ€§å†…å®¹
    const analyticalKeywords = [
      'åˆ†æ', 'è§£é‡Š', 'è¯´æ˜', 'æ€»ç»“', 'å½’çº³', 'æ¯”è¾ƒ', 'å¯¹æ¯”', 'è¯„ä¼°', 'åˆ¤æ–­',
      'analyze', 'explain', 'analyze', 'summarize', 'compare', 'evaluate',
      'ä¸ºä»€ä¹ˆ', 'å¦‚ä½•', 'æ€ä¹ˆ', 'åŸå› ', 'æ–¹æ³•', 'æ­¥éª¤', 'è¿‡ç¨‹', 'æœºåˆ¶'
    ];
    const isAnalytical = analyticalKeywords.some(keyword => content.includes(keyword));
    
    // è®¡ç®—å¤æ‚åº¦
    let complexity: 'low' | 'medium' | 'high' = 'low';
    if (hasCode && hasLongText) {
      complexity = 'high';
    } else if (hasCode || hasLongText || isAnalytical) {
      complexity = 'medium';
    }
    
    return {
      length,
      language,
      hasCode,
      hasLongText,
      isCreative,
      isAnalytical,
      complexity
    };
  }

  /**
   * æ ¹æ®å†…å®¹ç‰¹å¾é€‰æ‹©æœ€é€‚åˆçš„æ¨¡å‹
   */
  selectOptimalModel(content: string, context?: any): SupportedModel {
    const analysis = this.analyzeContent(content);
    
    // æ£€æŸ¥ç¼“å­˜
    const cacheKey = this.generateCacheKey(content, analysis);
    const cached = this.performanceCache.get(cacheKey);
    if (cached && Date.now() - cached.timestamp < this.cacheExpiryMs) {
      console.log(`[Model Router] ğŸ¯ ç¼“å­˜å‘½ä¸­: ${cached.model} (åˆ†æ•°: ${cached.score})`);
      return cached.model;
    }
    
    let selectedModel: SupportedModel = 'deepseek-r1'; // é»˜è®¤æ¨¡å‹
    let score = 0;
    
    // æ¨¡å‹é€‰æ‹©é€»è¾‘
    if (analysis.hasCode) {
      // ä»£ç ç›¸å…³ä»»åŠ¡ - DeepSeek R1 ä¼˜é€‰
      selectedModel = 'deepseek-r1';
      score = 0.9;
      console.log(`[Model Router] ğŸ”§ æ£€æµ‹åˆ°ä»£ç å†…å®¹ï¼Œé€‰æ‹© DeepSeek R1`);
    } else if (analysis.isCreative) {
      // åˆ›æ„æ€§ä»»åŠ¡ - GPT-4O ä¼˜é€‰
      selectedModel = 'chatgpt-4o-latest';
      score = 0.85;
      console.log(`[Model Router] ğŸ¨ æ£€æµ‹åˆ°åˆ›æ„å†…å®¹ï¼Œé€‰æ‹© GPT-4O`);
    } else if (analysis.hasLongText && analysis.isAnalytical) {
      // é•¿æ–‡æœ¬åˆ†æ - GPT-4O ä¼˜é€‰
      selectedModel = 'chatgpt-4o-latest';
      score = 0.8;
      console.log(`[Model Router] ğŸ“Š æ£€æµ‹åˆ°é•¿æ–‡æœ¬åˆ†æï¼Œé€‰æ‹© GPT-4O`);
    } else if (analysis.language === 'chinese' && analysis.complexity === 'high') {
      // é«˜å¤æ‚åº¦ä¸­æ–‡ä»»åŠ¡ - DeepSeek R1 ä¼˜é€‰
      selectedModel = 'deepseek-r1';
      score = 0.85;
      console.log(`[Model Router] ğŸ‡¨ğŸ‡³ æ£€æµ‹åˆ°é«˜å¤æ‚åº¦ä¸­æ–‡ä»»åŠ¡ï¼Œé€‰æ‹© DeepSeek R1`);
    } else if (analysis.length > 1000) {
      // è¶…é•¿æ–‡æœ¬ - GPT-4O ä¼˜é€‰
      selectedModel = 'chatgpt-4o-latest';
      score = 0.75;
      console.log(`[Model Router] ğŸ“ æ£€æµ‹åˆ°è¶…é•¿æ–‡æœ¬ï¼Œé€‰æ‹© GPT-4O`);
    } else {
      // é»˜è®¤æƒ…å†µ - DeepSeek R1
      selectedModel = 'deepseek-r1';
      score = 0.7;
      console.log(`[Model Router] ğŸ”„ é»˜è®¤é€‰æ‹© DeepSeek R1`);
    }
    
    // æ›´æ–°ç¼“å­˜
    this.performanceCache.set(cacheKey, {
      model: selectedModel,
      score,
      timestamp: Date.now()
    });
    
    // æ¸…ç†è¿‡æœŸç¼“å­˜
    this.cleanupCache();
    
    console.log(`[Model Router] âœ… æœ€ç»ˆé€‰æ‹©: ${selectedModel} (åˆ†æ•°: ${score}, å¤æ‚åº¦: ${analysis.complexity})`);
    return selectedModel;
  }

  /**
   * è·å–æ¨¡å‹ä¸“å±é…ç½®
   */
  getModelConfig(model: SupportedModel): ModelConfig {
    const configs: Record<SupportedModel, ModelConfig> = {
      'deepseek-r1': {
        temperature: 0.7,
        max_tokens: 8000,
        timeout: 25000,
        retry: 2,
        top_p: 0.95,
        frequency_penalty: 0,
        presence_penalty: 0,
        stream_options: { include_usage: true },
        flushThreshold: 1, // ç«‹å³å‘é€æ¯ä¸ªå­—ç¬¦
        flushInterval: 3,  // 3msé—´éš”ï¼Œæœ€å¿«æ¸²æŸ“
      },
      'chatgpt-4o-latest': {
        temperature: 0.8,
        max_tokens: 6000,
        timeout: 30000,
        retry: 3,
        top_p: 0.9,
        frequency_penalty: 0.1,
        presence_penalty: 0.1,
        stream_options: { include_usage: true },
        flushThreshold: 2,
        flushInterval: 8,
      },
      'claude-3-7-sonnet-latest': {
        temperature: 0.7,
        max_tokens: 4000,
        timeout: 25000,
        retry: 2,
        top_p: 0.9,
        frequency_penalty: 0,
        presence_penalty: 0,
        stream_options: { include_usage: true },
        flushThreshold: 3,
        flushInterval: 15,
      },
      'gemini-2.5-pro': {
        temperature: 0.7,
        max_tokens: 4000,
        timeout: 25000,
        retry: 2,
        top_p: 0.9,
        frequency_penalty: 0,
        presence_penalty: 0,
        stream_options: { include_usage: true },
        flushThreshold: 3,
        flushInterval: 20,
      },
    };
    
    const config = configs[model];
    console.log(`[Model Router] âš™ï¸ é…ç½® ${model}:`, {
      temperature: config.temperature,
      max_tokens: config.max_tokens,
      timeout: config.timeout,
      flushInterval: config.flushInterval
    });
    
    return config;
  }

  /**
   * æ ¹æ®å†å²æ€§èƒ½æ›´æ–°æ¨¡å‹åå¥½
   */
  updateModelPerformance(model: SupportedModel, contentHash: string, responseTime: number, success: boolean) {
    const score = success ? Math.max(0, 1 - responseTime / 60000) : 0; // 1åˆ†é’Ÿå†…å®Œæˆå¾—æ»¡åˆ†
    
    // æ›´æ–°ç¼“å­˜ä¸­çš„æ€§èƒ½åˆ†æ•°
    this.performanceCache.forEach((value, key) => {
      if (value.model === model && key.includes(contentHash)) {
        value.score = (value.score * 0.7) + (score * 0.3); // åŠ æƒå¹³å‡
        return;
      }
    });
    
    console.log(`[Model Router] ğŸ“Š æ›´æ–°æ€§èƒ½: ${model} - å“åº”æ—¶é—´: ${responseTime}ms, æˆåŠŸ: ${success}, åˆ†æ•°: ${score.toFixed(2)}`);
  }

  /**
   * è·å–æ¨¡å‹è´Ÿè½½å‡è¡¡å»ºè®®
   */
  getLoadBalancingRecommendation(): SupportedModel[] {
    const now = Date.now();
    const recentPerformance = new Map<SupportedModel, number[]>();
    
    // æ”¶é›†æœ€è¿‘çš„æ€§èƒ½æ•°æ®
    this.performanceCache.forEach((value, key) => {
      if (now - value.timestamp < 10 * 60 * 1000) { // 10åˆ†é’Ÿå†…çš„æ•°æ®
        if (!recentPerformance.has(value.model)) {
          recentPerformance.set(value.model, []);
        }
        recentPerformance.get(value.model)!.push(value.score);
      }
    });
    
    // è®¡ç®—å¹³å‡æ€§èƒ½å¹¶æ’åº
    const modelScores: Array<{ model: SupportedModel; avgScore: number }> = [];
    recentPerformance.forEach((scores, model) => {
      const avgScore = scores.reduce((a: number, b: number) => a + b, 0) / scores.length;
      modelScores.push({ model, avgScore });
    });
    
    // æŒ‰æ€§èƒ½æ’åº
    modelScores.sort((a, b) => b.avgScore - a.avgScore);
    
    const recommendation = modelScores.map(item => item.model);
    console.log(`[Model Router] ğŸ”„ è´Ÿè½½å‡è¡¡å»ºè®®:`, recommendation);
    
    return recommendation.length > 0 ? recommendation : ['deepseek-r1', 'chatgpt-4o-latest'];
  }

  /**
   * ç”Ÿæˆç¼“å­˜é”®
   */
  private generateCacheKey(content: string, analysis: ContentAnalysis): string {
    const contentHash = this.simpleHash(content.substring(0, 200)); // åªä½¿ç”¨å‰200å­—ç¬¦
    return `${contentHash}-${analysis.complexity}-${analysis.hasCode}-${analysis.language}`;
  }

  /**
   * ç®€å•å“ˆå¸Œå‡½æ•°
   */
  private simpleHash(str: string): string {
    let hash = 0;
    for (let i = 0; i < str.length; i++) {
      const char = str.charCodeAt(i);
      hash = ((hash << 5) - hash) + char;
      hash = hash & hash; // è½¬æ¢ä¸º32ä½æ•´æ•°
    }
    return Math.abs(hash).toString(16);
  }

  /**
   * æ¸…ç†è¿‡æœŸç¼“å­˜
   */
  private cleanupCache() {
    const now = Date.now();
    const keysToDelete: string[] = [];
    
    this.performanceCache.forEach((value, key) => {
      if (now - value.timestamp > this.cacheExpiryMs) {
        keysToDelete.push(key);
      }
    });
    
    keysToDelete.forEach(key => {
      this.performanceCache.delete(key);
    });
  }

  /**
   * è·å–ç»Ÿè®¡ä¿¡æ¯
   */
  getStats() {
    const stats = {
      cacheSize: this.performanceCache.size,
      modelDistribution: new Map<SupportedModel, number>(),
      avgPerformance: new Map<SupportedModel, number[]>(),
    };
    
    this.performanceCache.forEach((value, key) => {
      // ç»Ÿè®¡æ¨¡å‹åˆ†å¸ƒ
      stats.modelDistribution.set(
        value.model,
        (stats.modelDistribution.get(value.model) || 0) + 1
      );
      
      // è®¡ç®—å¹³å‡æ€§èƒ½
      const current = stats.avgPerformance.get(value.model) || [];
      stats.avgPerformance.set(value.model, [...current, value.score]);
    });
    
    // è®¡ç®—å¹³å‡å€¼
    const finalAvgPerformance = new Map<SupportedModel, number>();
    stats.avgPerformance.forEach((scores, model) => {
      const avg = scores.reduce((a: number, b: number) => a + b, 0) / scores.length;
      finalAvgPerformance.set(model, avg);
    });
    
    return {
      cacheSize: stats.cacheSize,
      modelDistribution: stats.modelDistribution,
      avgPerformance: finalAvgPerformance,
    };
  }
}

export default ModelRouter; 